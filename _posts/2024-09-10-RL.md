---
title: 'RL - Basic Knowledge List'
tags:
  - LLM
  - training
  - inference
  - fine-tuning
  - kernel
  - model architecture
  - hardware
---

This post summarizes some extremely basic and old knowledge about Reinforcement Learning (RL) 

* TOC 
{:toc}


# traditional RL

## preliminary: Markov Decision Process (MDP)

- markov property
- stationary distribution

## problem definition & terminology

- state at step 
  $$
  t
  $$
  , 
  $$
  s_t
  $$
  
- action at step 
  $$
  t
  $$
  , 
  $$
  a_t
  $$
  
- reward of a step 
  $$
  r(s_t, a_t)
  $$
  
  - the reward function in RL is always with a clear definition and an accurate computation
    - example1: in AlphaGo Zero, the reward function is zero for all non-terminal steps, and +1 for winning -1 for losing at the end of the game.
    - example2: in RLHF, the reward network is additionally trained in advance from the human feedback dataset
  
- discount factor 
  $$
  \gamma \in (0, 1]
  $$
  
- cumulative discounted reward after step 
  $$
  t
  $$
  , 
  $$
  G_t
  $$

  - state & state-action value functions V(s), Q(s,a)

- state transition
  $$
   p(s' | s, a)
  $$
  
- policy 
  $$
  \pi(a | s)
  $$

## methods

### dynamic programming

- Bellman equations of value functions V(s), Q(s,a)

- policy evaluation & improvement

### Monte Carlo

- Monte Carlo (MC) estimation of value functions from sampled experience

### temporal difference

- temporal difference (TD)
  - MC + Bellman 
    - using state & state-action values of future time steps 
    - TD(0), TD(
    $$
    \lambda
    $$
    ​	)

  - TD methods
    - SARSA
    - Q-learning
      - DQN (a case of Deep RL)
        - beyond traditional tabular Q(s, a), DQN uses function approximated Q(s, a)




# Deep RL (traditional RL + DL)

## methods

### on-policy 

- policy gradient theorem
  - subtracting baseline to reduce variance
  
- REINFORCE 
  
  - MC estimation of 
    $$
    G_t
    $$
  
  - cases: 
    $$
    \gamma = 1
    $$
    , 
    $$
    \gamma < 1
    $$
  
- actor-critic
  - beyond REINFORCE, actor-critic learns a parameterized value function with a TD loss 

### off-policy 

note: basically irrelavant to the conclusions of on-policy methods like policy gradient theorem

- TRPO
  - policy performance η(π ̃)−η(π) can be decomposed as a sum of per-timestep advantages.
  - importance sampling
- PPO
  - 3 loss items:
    - TRPO learning objective with penalty / clip
    - TD loss to train value function
    - policy entropy loss for exploration
