---
title: 'RL - Basic Knowledge List'
tags:
  - LLM
  - training
  - inference
  - fine-tuning
  - kernel
  - model architecture
  - hardware
---

This post summarizes the extremely basic knowledge about Reinforcement Learning (RL) 

* TOC 
{:toc}


## traditional RL

### preliminary: Markov Decision Process (MDP)

- markov property
- stationary distribution

### problem definition & terminology

- state at step 
  $$
  t
  $$
  , 
  $$
  s_t
  $$
  
- action at step 
  $$
  t
  $$
  , 
  $$
  a_t
  $$
  
- reward of a step 
  $$
  r(s_t, a_t)
  $$
  

  - example1: in AlphaGo Zero, the reward function is zero for all non-terminal steps, and +1 for winning -1 for losing at the end of the game.
  - example2: in RLHF, the reward network is additionally trained in advance from the human feedback dataset

- discount factor 
  $$
  \gamma \in (0, 1]
  $$
  
- cumulative discounted reward after step 
  $$
  t
  $$
  , 
  $$
  G_t
  $$

  - state & state-action value functions V(s), Q(s,a)

- state transition
  $$
   p(s' | s, a)
  $$
  
- policy: 
  $$
  \pi(a | s)
  $$

### basic methods

- bellman equations of value functions V(s), Q(s,a)

- policy evaluation & improvement

- monte carlo (MC) estimation of value functions from experience (a.k.a. observation)

- time difference (TD)
  - MC + bellman 
    - using state & state-action values of future time steps
    - TD(0), TD(
    $$
    \lambda
    $$
    ​	)

  - TD methods
    - SARSA
    - Q-learning
      - DQN (a case of Deep RL)
        - beyond traditional tabular Q(s, a), DQN uses function approximated Q(s, a)




## Deep RL (traditional RL + DL)

### policy gradient methods

- policy gradient theorem
  - using baseline to reduce variance
- REINFORCE algorithm
- actor-critic
  - learning value function with a TD loss 

### off-policy methods

note: irrelavant to policy gradient theorem

- TPRO
  - policy performance η(π ̃)−η(π) can be decomposed as a sum of per-timestep advantages.
  - importance sampling
- PPO
  - penalty
  - clip
