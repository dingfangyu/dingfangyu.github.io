---
title: 'Cluster-Aware Neural Temporal Point Process'
date: 2023-06-26
permalink: /posts/2023/06/cntpp/
tags:
  - cool posts
  - category1
  - category2
---

<!-- ## Cluster-Aware Neural Temporal Point Process -->

## Preliminaries 

- Temporal Point Process (TPP) is a mathematical tool to model the probability of event sequence data  $S = \{t_i\}_{i=1}^{N}$ on a timespan $[0, T]$, where each event $i$ has its timestamp $t_i$. The probability is calculated based on the conditional intensity function $\lambda^*(t)$: $$P(S) = \prod_{i=1}^{N}\lambda^*(t_i)\exp[-\int_0^T \lambda^*(\tau) \mathrm{d}\tau],$$ and the event information can be enriched, beyond the simple case that only contains the timestamp.

- Variational Autoencoder (VAE) is a probabilistic model for latent variable ($z$) inference given the observation ($x$), and the latent variable model could be used for clustering by inferring the cluster index $z$. The optimization objective (ELBO) of VAE is as follow: $$\text{ELBO} = \mathbb{E}_{z \sim q(z\vert x)}[\log p(x\vert z)] - \text{KL}[q(z\vert x) \Vert   p(z)],$$ where $q(z\vert x)$ is the posterior, $p(x\vert z)$ is the likelihood, and $p(z)$ is the prior. VAE is learned as an encoder-decoder architecture, where $q(z\vert x)$ is the encoder from $x$ to $z$, and $p(x\vert z)$ is the decoder from $z$ to $x$.

## Motivation

- The event sequence data always has inherent sparse structure, an event is more likely to be highly correlated with a small subset of the history events, which leads to the motivation of clustering within the event sequence (sequence decomposition). ![demo](https://github.com/Arthur-99/Arthur-99.github.io/tree/master/_posts/cluster.png)
- The sequence decomposition problem can be formulated as a latent variable inference problem, which could be solved via a VAE.

## Methodology

- We propose a fully differentiable VAE-based deep clustering framework for sequential data, and (neural) temporal point process (TPP) as a mathematical tool is used to model the probabilitiy of sequential data. With such a framework an event sequence can be decomposed to different highly correlated subsequences (i.e. clusters) unsupervisedly, and the encoder (inference, or posterior), regularization (or prior), decoder (reconstruction, or likelihood), and learning objective (or the ELBO in VAE context) can be abstracted as below: 

$$\begin{align*}q(Z\vert X) &= \text{Seq2Seq}(X),\\p(Z)   &= \text{Uniform}(K),\\p(X\vert Z) &= \prod_{k=1}^K \text{TPP}(X_k),\\\text{ELBO} &= \mathbb{E}_{Z \sim q(Z\vert X)}[\log p(X\vert Z)] - \text{KL}[q(Z\vert X) \Vert   p(Z)],\end{align*}$$ 
where X is the input sequence, Z is the categorical 1-of-K decomposition scheme, and X_k is the k-th subsequence.
![](https://github.com/Arthur-99/Arthur-99.github.io/tree/master/_posts/demo.png)